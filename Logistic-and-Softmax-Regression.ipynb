{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSX0E8XEdlE6"
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uHoY8iSWdocP"
   },
   "outputs": [],
   "source": [
    "#------------ No New Package --------------\n",
    "import numpy as np\n",
    "#---------------------------------------------------------\n",
    "\n",
    "class LogisticRegression:\n",
    "    \"\"\"\n",
    "    Logistic Regression classifier using Stochastic Gradient Descent (SGD)\n",
    "    for binary classification problems.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate=0.01, n_epochs=100, batch_size=32):\n",
    "        \"\"\"\n",
    "        Initialize the Logistic Regression model.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        learning_rate : float\n",
    "            The step size for updating model parameters\n",
    "        n_epochs : int\n",
    "            Number of passes through the training data\n",
    "        batch_size : int\n",
    "            Number of training examples to use in each gradient update\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.w = None  # weights\n",
    "        self.b = None  # bias\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Compute the sigmoid activation function.\n",
    "\n",
    "        Formula:\n",
    "        σ(z) = 1 / (1 + e^(-z))\n",
    "        where z = wx + b\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        z : array-like\n",
    "            Input values\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        array-like\n",
    "            Sigmoid of input values\n",
    "        \"\"\"\n",
    "        return 1/(1+ np.exp(-z))\n",
    "\n",
    "    def initialize_parameters(self, n_features):\n",
    "        \"\"\"\n",
    "        Initialize model parameters using Xavier initialization.\n",
    "\n",
    "        Formula for Xavier initialization:\n",
    "        w ~ N(0, sqrt(2/n_features))\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_features : int\n",
    "            Number of input features\n",
    "        \"\"\"\n",
    "        # Xavier initialization for better convergence\n",
    "        self.w = np.random.randn(n_features) * np.sqrt(2.0 / n_features)\n",
    "        self.b = 0\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute binary cross-entropy loss.\n",
    "\n",
    "        Formula:\n",
    "        L = -1/N * Σ(y * log(ŷ) + (1-y) * log(1-ŷ))\n",
    "        where:\n",
    "        - y is true label\n",
    "        - ŷ is predicted probability\n",
    "        - N is number of samples\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        y_true : array-like\n",
    "            True binary labels  \n",
    "        y_pred : array-like\n",
    "            Predicted probabilities\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Average binary cross-entropy loss\n",
    "        \"\"\"\n",
    "     \n",
    "        loss = -np.mean(y_true * np.log(y_pred + 1e-15) + (1 - y_true) * np.log(1 - y_pred + 1e-15))\n",
    "        return loss\n",
    "\n",
    "    def compute_gradients(self, X_batch, y_batch, y_pred):\n",
    "        \"\"\"\n",
    "        Compute gradients for weights and bias.\n",
    "\n",
    "        Formulas:\n",
    "        ∂L/∂w = 1/N * X^T * (ŷ - y)\n",
    "        ∂L/∂b = 1/N * Σ(ŷ - y)\n",
    "        where:\n",
    "        - X is input features\n",
    "        - y is true labels\n",
    "        - ŷ is predicted probabilities\n",
    "        - N is batch size\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_batch : array-like\n",
    "            Input features for current batch\n",
    "        y_batch : array-like\n",
    "            True labels for current batch\n",
    "        y_pred : array-like\n",
    "            Predicted probabilities for current batch\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            Gradients for weights and bias\n",
    "        \"\"\"\n",
    "        n = y_batch.shape[0]\n",
    "        error = y_pred - y_batch \n",
    "        dw = (1/n)* np.dot(X_batch.T, error)\n",
    "        db = (1/n) * np.sum(error)\n",
    "        return dw, db\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the logistic regression model using mini-batch SGD.\n",
    "\n",
    "        Process:\n",
    "        1. Initialize parameters\n",
    "        2. For each epoch:\n",
    "            a. Shuffle data\n",
    "            b. Split into mini-batches\n",
    "            c. For each mini-batch:\n",
    "                - Compute forward pass (sigmoid)\n",
    "                - Compute gradients\n",
    "                - Update parameters\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Target values\n",
    "        \"\"\"\n",
    "        # Initialize Parameters\n",
    "        n_samples, n_features = X.shape\n",
    "        self.initialize_parameters(n_features)\n",
    "\n",
    "        # Epoch Loop\n",
    "        for epoch in range(self.n_epochs):\n",
    "            # Shuffle the data\n",
    "            indices = np.arange(n_samples)\n",
    "            np.random.shuffle(indices)\n",
    "            X = X[indices]\n",
    "            y = y[indices]\n",
    "\n",
    "            # Mini-batch training\n",
    "            for i in range(0, n_samples, self.batch_size):\n",
    "                \n",
    "                # Get Batch Data\n",
    "                X_batch = X[i:i + self.batch_size]\n",
    "                y_batch = y[i:i + self.batch_size]\n",
    "\n",
    "                # Forward pass\n",
    "                z = np.dot(X_batch, self.w) + self.b\n",
    "                y_pred = self.sigmoid(z)\n",
    "\n",
    "                # Compute gradients\n",
    "                dw, db = self.compute_gradients(X_batch, y_batch, y_pred)\n",
    "\n",
    "                # Update parameters\n",
    "                self.w -= self.learning_rate * dw\n",
    "                self.b -= self.learning_rate * db       \n",
    "\n",
    "            # Calculate and trace the loss\n",
    "            pred = self.sigmoid(np.dot(X, self.w) + self.b)\n",
    "            loss = self.compute_loss(y, pred)\n",
    "            print(f\"Epoch {epoch + 1}/{self.n_epochs}, Loss: {loss}\")\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict class probabilities for input samples.\n",
    "\n",
    "        Formula:\n",
    "        P(y=1|x) = σ(wx + b)\n",
    "        where σ is the sigmoid function\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Input samples\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        array-like of shape (n_samples,)\n",
    "            Predicted probabilities\n",
    "        \"\"\"\n",
    "        z = np.dot(X, self.w) + self.b\n",
    "        return self.sigmoid(z)\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Predict class labels for input samples.\n",
    "\n",
    "        Formula:\n",
    "        y = 1 if P(y=1|x) >= threshold else 0\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Input samples\n",
    "        threshold : float\n",
    "            Classification threshold\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        array-like of shape (n_samples,)\n",
    "            Predicted class labels (0 or 1)\n",
    "        \"\"\"\n",
    "        y_prob = self.predict_proba(X)\n",
    "        return (y_prob >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vIH2qghmeak3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.7710158504566229\n",
      "Epoch 2/100, Loss: 0.7272705759876165\n",
      "Epoch 3/100, Loss: 0.6873079361502843\n",
      "Epoch 4/100, Loss: 0.6508780694633292\n",
      "Epoch 5/100, Loss: 0.6177744178187602\n",
      "Epoch 6/100, Loss: 0.5877167692359556\n",
      "Epoch 7/100, Loss: 0.5604594821965195\n",
      "Epoch 8/100, Loss: 0.535702362205049\n",
      "Epoch 9/100, Loss: 0.5132625705296096\n",
      "Epoch 10/100, Loss: 0.4928812983677808\n",
      "Epoch 11/100, Loss: 0.47434697959347305\n",
      "Epoch 12/100, Loss: 0.45746430459759746\n",
      "Epoch 13/100, Loss: 0.44208171557269454\n",
      "Epoch 14/100, Loss: 0.4280448218921059\n",
      "Epoch 15/100, Loss: 0.41518947933501144\n",
      "Epoch 16/100, Loss: 0.40339282918858976\n",
      "Epoch 17/100, Loss: 0.39255358824001746\n",
      "Epoch 18/100, Loss: 0.38256620562995663\n",
      "Epoch 19/100, Loss: 0.37334290006042165\n",
      "Epoch 20/100, Loss: 0.3648011974227194\n",
      "Epoch 21/100, Loss: 0.35688138169906103\n",
      "Epoch 22/100, Loss: 0.3495218492253023\n",
      "Epoch 23/100, Loss: 0.3426588713511623\n",
      "Epoch 24/100, Loss: 0.336255078502004\n",
      "Epoch 25/100, Loss: 0.3302583139781538\n",
      "Epoch 26/100, Loss: 0.32463802091007277\n",
      "Epoch 27/100, Loss: 0.31935396192970755\n",
      "Epoch 28/100, Loss: 0.3143870795603246\n",
      "Epoch 29/100, Loss: 0.3097046242008523\n",
      "Epoch 30/100, Loss: 0.30527679393726975\n",
      "Epoch 31/100, Loss: 0.30108415731255045\n",
      "Epoch 32/100, Loss: 0.2971080027206513\n",
      "Epoch 33/100, Loss: 0.2933359900033434\n",
      "Epoch 34/100, Loss: 0.28974337675598894\n",
      "Epoch 35/100, Loss: 0.28632297653119076\n",
      "Epoch 36/100, Loss: 0.28305952628784037\n",
      "Epoch 37/100, Loss: 0.27994144792070036\n",
      "Epoch 38/100, Loss: 0.2769577411778702\n",
      "Epoch 39/100, Loss: 0.2741009149675083\n",
      "Epoch 40/100, Loss: 0.271359749089047\n",
      "Epoch 41/100, Loss: 0.2687280342642204\n",
      "Epoch 42/100, Loss: 0.26619780260447984\n",
      "Epoch 43/100, Loss: 0.2637640927254266\n",
      "Epoch 44/100, Loss: 0.2614174482753456\n",
      "Epoch 45/100, Loss: 0.2591555466685684\n",
      "Epoch 46/100, Loss: 0.25697147238903467\n",
      "Epoch 47/100, Loss: 0.2548619383220922\n",
      "Epoch 48/100, Loss: 0.2528222202216442\n",
      "Epoch 49/100, Loss: 0.2508490229448267\n",
      "Epoch 50/100, Loss: 0.2489375207252068\n",
      "Epoch 51/100, Loss: 0.24708545740559365\n",
      "Epoch 52/100, Loss: 0.24528833040380632\n",
      "Epoch 53/100, Loss: 0.24354422695215633\n",
      "Epoch 54/100, Loss: 0.24184923885567378\n",
      "Epoch 55/100, Loss: 0.24020183541069584\n",
      "Epoch 56/100, Loss: 0.23859945573500427\n",
      "Epoch 57/100, Loss: 0.23703986541177063\n",
      "Epoch 58/100, Loss: 0.23552092341989322\n",
      "Epoch 59/100, Loss: 0.2340415735242943\n",
      "Epoch 60/100, Loss: 0.23260012139131866\n",
      "Epoch 61/100, Loss: 0.23119400052237526\n",
      "Epoch 62/100, Loss: 0.22982227346640718\n",
      "Epoch 63/100, Loss: 0.2284822854027507\n",
      "Epoch 64/100, Loss: 0.22717360029456185\n",
      "Epoch 65/100, Loss: 0.2258955107495738\n",
      "Epoch 66/100, Loss: 0.22464601956049712\n",
      "Epoch 67/100, Loss: 0.22342421959939276\n",
      "Epoch 68/100, Loss: 0.2222290883243096\n",
      "Epoch 69/100, Loss: 0.22105902720512835\n",
      "Epoch 70/100, Loss: 0.21991434586733127\n",
      "Epoch 71/100, Loss: 0.21879286394935377\n",
      "Epoch 72/100, Loss: 0.21769410123739508\n",
      "Epoch 73/100, Loss: 0.21661726037551404\n",
      "Epoch 74/100, Loss: 0.21556159071102407\n",
      "Epoch 75/100, Loss: 0.21452614071470413\n",
      "Epoch 76/100, Loss: 0.21351078379649713\n",
      "Epoch 77/100, Loss: 0.2125149667371503\n",
      "Epoch 78/100, Loss: 0.21153724850676744\n",
      "Epoch 79/100, Loss: 0.2105774683638332\n",
      "Epoch 80/100, Loss: 0.20963474852769576\n",
      "Epoch 81/100, Loss: 0.20870885966062833\n",
      "Epoch 82/100, Loss: 0.20779909611187455\n",
      "Epoch 83/100, Loss: 0.20690532714976645\n",
      "Epoch 84/100, Loss: 0.20602646349416204\n",
      "Epoch 85/100, Loss: 0.20516259910467716\n",
      "Epoch 86/100, Loss: 0.20431296126886028\n",
      "Epoch 87/100, Loss: 0.2034772399978991\n",
      "Epoch 88/100, Loss: 0.20265503429408654\n",
      "Epoch 89/100, Loss: 0.20184586396384824\n",
      "Epoch 90/100, Loss: 0.20104980573165626\n",
      "Epoch 91/100, Loss: 0.2002659294869565\n",
      "Epoch 92/100, Loss: 0.1994940859237542\n",
      "Epoch 93/100, Loss: 0.19873411897077575\n",
      "Epoch 94/100, Loss: 0.19798553242228586\n",
      "Epoch 95/100, Loss: 0.19724815246979538\n",
      "Epoch 96/100, Loss: 0.19652177214198885\n",
      "Epoch 97/100, Loss: 0.19580574883717677\n",
      "Epoch 98/100, Loss: 0.19509995624543577\n",
      "Epoch 99/100, Loss: 0.1944042546620582\n",
      "Epoch 100/100, Loss: 0.19371842026690594\n",
      "Accuracy: 0.98\n",
      "Confusion Matrix:\n",
      "[[ 63   2]\n",
      " [  2 133]]\n"
     ]
    }
   ],
   "source": [
    "# Test Code\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)  # For reproducibility\n",
    "n_samples = 1000\n",
    "n_features = 10\n",
    "\n",
    "# Generate random features\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "\n",
    "# Generate binary target labels (0 or 1)\n",
    "# Let's create labels based on a linear combination of features plus some noise\n",
    "weights = np.random.randn(n_features)\n",
    "bias = np.random.randn(1)\n",
    "y_prob = 1 / (1 + np.exp(-(np.dot(X, weights) + bias)))  # Sigmoid to get probabilities\n",
    "y = (y_prob > 0.5).astype(int).flatten()  # Convert probabilities to binary labels\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and fit the Logistic Regression model\n",
    "model = LogisticRegression(learning_rate=0.01, n_epochs=100, batch_size=32)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Regression (Multinomial Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SoftmaxRegression:\n",
    "    \"\"\"\n",
    "    Softmax Regression classifier using Stochastic Gradient Descent (SGD)\n",
    "    for multiclass classification problems.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate=0.01, n_epochs=100, batch_size=32):\n",
    "        \"\"\"\n",
    "        Initialize the Softmax Regression model.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        learning_rate : float\n",
    "            The step size for updating model parameters\n",
    "        n_epochs : int\n",
    "            Number of passes through the training data\n",
    "        batch_size : int\n",
    "            Number of training examples to use in each gradient update\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.W = None  # weights\n",
    "        self.b = None  # biases\n",
    "\n",
    "    def softmax(self, z):\n",
    "        \"\"\"\n",
    "        Compute the softmax function.\n",
    "\n",
    "        Formula:\n",
    "        softmax(z) = exp(z) / Σ exp(z)\n",
    "        where z = Wx + b\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        z : array-like of shape (n_samples, n_classes)\n",
    "            Input values\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        array-like of shape (n_samples, n_classes)\n",
    "            Softmax probabilities for each class\n",
    "        \"\"\"\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  \n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def initialize_parameters(self, n_features, n_classes):\n",
    "        \"\"\"\n",
    "        Initialize model parameters using Xavier initialization.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_features : int\n",
    "            Number of input features\n",
    "        n_classes : int\n",
    "            Number of output classes\n",
    "        \"\"\"\n",
    "        # Xavier initialization for weights\n",
    "        self.W = np.random.randn(n_features, n_classes) * np.sqrt(2.0 / n_features)\n",
    "        self.b = np.zeros((1, n_classes))\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute cross-entropy loss for multiclass classification.\n",
    "\n",
    "        Formula:\n",
    "        L = -1/N * Σ Σ(y * log(ŷ))\n",
    "        where:\n",
    "        - y is true label\n",
    "        - ŷ is predicted probability\n",
    "        - N is number of samples\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        y_true : array-like of shape (n_samples, n_classes)\n",
    "            True one-hot encoded labels\n",
    "        y_pred : array-like of shape (n_samples, n_classes)\n",
    "            Predicted probabilities\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Average cross-entropy loss\n",
    "        \"\"\"\n",
    "        m = y_true.shape[0]\n",
    "        loss = -np.sum(y_true * np.log(y_pred + 1e-15)) / m\n",
    "        return loss\n",
    "\n",
    "    def compute_gradients(self, X_batch, y_batch, y_pred):\n",
    "        \"\"\"\n",
    "        Compute gradients for weights and biases.\n",
    "\n",
    "        Formulas:\n",
    "        ∂L/∂W = 1/N * X^T * (ŷ - y)\n",
    "        ∂L/∂b = 1/N * Σ(ŷ - y)\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_batch : array-like\n",
    "            Input features for current batch\n",
    "        y_batch : array-like\n",
    "            True labels for current batch\n",
    "        y_pred : array-like\n",
    "            Predicted probabilities for current batch\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            Gradients for weights and biases\n",
    "        \"\"\"\n",
    "        m = X_batch.shape[0]\n",
    "        dW = (1 / m) * np.dot(X_batch.T, (y_pred - y_batch))\n",
    "        db = (1 / m) * np.sum(y_pred - y_batch, axis=0, keepdims=True)\n",
    "        return dW, db\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the softmax regression model using mini-batch SGD.\n",
    "\n",
    "        Process:\n",
    "        1. Initialize parameters\n",
    "        2. For each epoch:\n",
    "            a. Shuffle data\n",
    "            b. Split into mini-batches\n",
    "            c. For each mini-batch:\n",
    "                - Compute forward pass (softmax)\n",
    "                - Compute gradients\n",
    "                - Update parameters\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Target values (class labels)\n",
    "        \"\"\"\n",
    "        # Initialize Parameters\n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = y.shape[1]\n",
    "        \n",
    "        self.initialize_parameters(n_features, n_classes)\n",
    "\n",
    "        # Epoch Loop\n",
    "        for epoch in range(self.n_epochs):\n",
    "            # Shuffle the data\n",
    "            indices = np.arange(n_samples)\n",
    "            np.random.shuffle(indices)\n",
    "            X = X[indices]\n",
    "            y = y[indices]\n",
    "\n",
    "            # Mini-batch training\n",
    "            for i in range(0, n_samples, self.batch_size):\n",
    "                # Get Batch Data\n",
    "                X_batch = X[i:i + self.batch_size]\n",
    "                y_batch = y[i:i + self.batch_size]\n",
    "\n",
    "                # Forward pass\n",
    "                z = np.dot(X_batch, self.W) + self.b\n",
    "                y_pred = self.softmax(z)\n",
    "\n",
    "                # Compute gradients\n",
    "                dW, db = self.compute_gradients(X_batch, y_batch, y_pred)\n",
    "\n",
    "                # Update parameters\n",
    "                self.W -= self.learning_rate * dW\n",
    "                self.b -= self.learning_rate * db\n",
    "\n",
    "            # Calculate and trace the loss\n",
    "        pred = self.softmax(np.dot(X, self.W) + self.b)\n",
    "        loss = self.compute_loss(y, pred)\n",
    "        print(f\"Epoch {epoch + 1}/{self.n_epochs}, Loss: {loss}\")\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict class probabilities for input samples.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Input samples\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        array-like of shape (n_samples, n_classes)\n",
    "            Predicted probabilities for each class\n",
    "        \"\"\"\n",
    "        z = np.dot(X, self.W) + self.b\n",
    "        return self.softmax(z)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for input samples.\n",
    "\n",
    "        Formula:\n",
    "        y = argmax(P(y|x))\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Input samples\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        array-like of shape (n_samples,)\n",
    "            Predicted class labels\n",
    "        \"\"\"\n",
    "        y_proba = self.predict_proba(X)\n",
    "        return np.argmax(y_proba, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Loss: 0.41327039373727276\n",
      "Accuracy: 0.8346\n",
      "Confusion Matrix:\n",
      "[[815   2  37  89   3   2  40   0  12   0]\n",
      " [  3 952   6  31   4   0   2   0   2   0]\n",
      " [ 20   3 864  18  69   1  15   1   9   0]\n",
      " [ 21   8  29 906  18   0  12   0   6   0]\n",
      " [  0   0 228  59 676   0  32   0   5   0]\n",
      " [  0   0   0   1   0 935   0  41   4  19]\n",
      " [153   2 237  85 111   1 385   0  26   0]\n",
      " [  0   0   0   0   0  43   0 911   0  46]\n",
      " [  6   1  18  14   1   4   6   6 944   0]\n",
      " [  0   0   0   0   0  16   0  25   1 958]]\n"
     ]
    }
   ],
   "source": [
    "# Test Code\n",
    "import numpy as np\n",
    "import requests\n",
    "import gzip\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# URLs to download the Fashion MNIST dataset\n",
    "BASE_URL = \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/\"\n",
    "FILENAMES = {\n",
    "    \"train_images\": \"train-images-idx3-ubyte.gz\",\n",
    "    \"train_labels\": \"train-labels-idx1-ubyte.gz\",\n",
    "    \"test_images\": \"t10k-images-idx3-ubyte.gz\",\n",
    "    \"test_labels\": \"t10k-labels-idx1-ubyte.gz\",\n",
    "}\n",
    "\n",
    "# Download and extract Fashion MNIST dataset\n",
    "def download_and_load_mnist(filename, num_items, item_size, reshape_dims=None):\n",
    "    if not os.path.exists(filename):\n",
    "        urlretrieve(BASE_URL + filename.split(\"/\")[-1], filename)\n",
    "\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8, offset=item_size)\n",
    "        if reshape_dims:\n",
    "            return data.reshape(num_items, *reshape_dims)\n",
    "        else:\n",
    "            return data\n",
    "\n",
    "# Load datasets\n",
    "def load_fashion_mnist():\n",
    "    train_images = download_and_load_mnist(\"train-images-idx3-ubyte.gz\", 60000, 16, (28, 28))\n",
    "    train_labels = download_and_load_mnist(\"train-labels-idx1-ubyte.gz\", 60000, 8)\n",
    "    test_images = download_and_load_mnist(\"t10k-images-idx3-ubyte.gz\", 10000, 16, (28, 28))\n",
    "    test_labels = download_and_load_mnist(\"t10k-labels-idx1-ubyte.gz\", 10000, 8)\n",
    "\n",
    "    # Reshape and normalize images\n",
    "    train_images = train_images.reshape(60000, 28*28) / 255.0\n",
    "    test_images = test_images.reshape(10000, 28*28) / 255.0\n",
    "\n",
    "    return train_images, train_labels, test_images, test_labels\n",
    "\n",
    "# One-hot encoding for labels\n",
    "def one_hot_encode(labels, num_classes=10):\n",
    "    encoder = OneHotEncoder(categories=[range(num_classes)], sparse_output=False)\n",
    "    return encoder.fit_transform(labels.reshape(-1, 1))\n",
    "\n",
    "# Load data\n",
    "X_train, y_train, X_test, y_test = load_fashion_mnist()\n",
    "y_train_encoded = one_hot_encode(y_train)\n",
    "y_test_encoded = one_hot_encode(y_test)\n",
    "\n",
    "# Initialize and train the Softmax Regression model\n",
    "model = SoftmaxRegression(learning_rate=0.1, n_epochs=50, batch_size=64)\n",
    "model.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1 (v3.11.1:a7a450f84a, Dec  6 2022, 15:24:06) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
